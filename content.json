{"posts":[{"title":"Paper Reading 1 - Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation","text":"Problem introduction The paper introduced an optimization problem to learn a graph from signals that are assumed to be smooth and admitting a sparse representation in the spectral domain of the graph. What is graph learningConsidering an undirected and weighted graph G=($\\mathcal{V}$,$\\mathcal{E}$) with no self-loops, where $\\mathcal{V}=\\lbrace 1,\\dots,N \\rbrace$ represent the node and $\\mathcal{E}=\\lbrace (i, j, w_{i j}), i, j \\in \\mathcal{V}\\rbrace$ represent the edge with weights $w_{i j}$.The paper focus on learning the Laplacian matrix $L$(Symmetric positive semi-define matrix)$=D - W$, where $D$ is the diagonal degree matrix and $W$ the weight matrix. $Lâ€™s$ eigenvalue decomposition is $L=X \\Lambda X^{\\top}$ with $\\Lambda=\\operatorname{diag}\\left(\\lambda_1, \\ldots, \\lambda_N\\right)$ a diagonal matrix with the eigenvalues and $X=\\left(x_1, \\ldots, x_N\\right)$ a matrix with the eigenvectors as columns.A graph signal is defined as a function $y: \\mathcal{V} \\rightarrow \\mathbb{R}^N$ that assigns a scalar value to each vertex. The two concepts of smoothness and spectral representation in graph learning problems are explained as follows: SmoothnessA graph signal $y$ is s-smooth with respect to the graph:\\begin{equation} y^{\\top} L y=\\frac{1}{2} \\sum_{i, j} w_{i j}\\left(y_i-y_j\\right)^2 \\leq s\\tag{1} \\label{smooth} \\end{equation} where $s\\ge 0$ is smoothness level. Spectral sparsityA graph signal $y$ admits a $k$-sparse spectral representation with respect to the graph: $$|h|_0 \\leq k$$ where $h=X^Ty$ for Graph Fourier Transform. From an intuitive point of view, smoothness means that the signal values between adjacent nodes are close, and k-sparse spectral representation means that in the overall graph, it can be divided into k clusters, and the signal values are smooth within the cluster and vary greatly between clusters. Abstract optimization problem $$\\min _{H, X, \\Lambda}|Y-X H|_F^2+\\alpha\\left|\\Lambda^{1 / 2} H\\right|_F^2+\\beta|H|_S,\\\\s.t. \\begin{cases}X^{\\top} X=I_N, x_1=\\frac{1}{\\sqrt{N}} \\mathbf{1}_N, \\text { (a) } \\\\(X \\Lambda X^{\\top})_{k,\\ell} \\leq 0 \\quad k \\neq \\ell, \\text { (b) } \\\\\\Lambda=\\operatorname{diag} (0, \\lambda_2, \\ldots, \\lambda_N) \\succeq 0, \\text { (c) } \\\\\\operatorname{tr}(\\Lambda)=N \\in \\mathbb{R}_{*}^{+} \\text { (d) } \\\\\\end{cases}$$ $|Y-X H|_F^2$ controls the distance of the new representation $XH$ to the observations $Y$. From equation $(1)$, $\\sum_i y^{(i) \\top} L y^{(i)}=\\operatorname{tr}\\left(Y^{\\top} L Y\\right)=\\left|L^{1 / 2} Y\\right|_F^2$, then replacing $Y$ with $XH$, obtaing $\\left|L^{1 / 2} X H\\right|_F^2=\\left|X \\Lambda^{1 / 2} X^{\\top} X H\\right|_F^2=\\left|X\\right|_F^2 \\left|\\Lambda^{1 / 2} H\\right|_F^2=\\operatorname{tr}(X^TX)\\left|\\Lambda^{1 / 2} H\\right|_F^2=\\left|\\Lambda^{1 / 2} H\\right|_F^2$. This objective item controls the smoothness of the new representation. $\\beta|H|_S$ is a sparsity regularization (as $h=X^Ty=X^{-1}y\\Rightarrow Xh=y \\Rightarrow XH \\rightarrow Y$). The $|\\cdot|_S$ here is often expressed as $\\mathcal{l}_{1,2}$ (the sum of the $\\mathcal{l}_{2}$-norm of each row of $H$) in the paper. Constraints (a), (b), (c) originate from the properties of Laplacian matrix $L$. Constraint (d) was as to impose structure in the learned graph while avoiding that the trivial solution $\\widehat{\\Lambda}=0$. Reformulation of the problem Reformulation of the constraint(a)Given $X, X_0 \\in \\mathbb{R}^{N \\times N}$ two orthogonal matrices, both having their first column equal to $\\frac{1}{\\sqrt{N}}\\mathbf{1}_N$. And $$ X=X_0 \\left[\\begin{array}{cc} 1 &amp; \\mathbf{0}_{N-1}^{\\top} \\\\ \\mathbf{0}_{N-1} &amp; {[X_0^{\\top} X]_{2:, 2:}} \\end{array}\\right] $$ where $[X_0^{\\top} X]_{2:, 2:} := U $ is in $\\operatorname{Orth}(N-1)=\\lbrace X \\in \\mathbb{R}^{N-1 \\times N-1} \\mid X^{\\top} X=I_N \\rbrace$. Constraint (a) is still satisfied after the above reformulation, and the optimization problem is transformed into the following:$$\\min_{H, U, \\Lambda}\\left|Y-X_0\\left[\\begin{array}{cc}1 &amp; \\mathbf{0}_{N-1}^{\\top} \\\\ \\mathbf{0}_{N-1} &amp; U\\end{array}\\right] H\\right|_F^2+\\alpha\\left|\\Lambda^{1 / 2} H\\right|_F^2+\\beta|H|_S \\triangleq f(H, U, \\Lambda) \\\\s.t. \\begin{cases}U^{\\top} U=I_{N-1}, \\quad ( a^{\\prime} )\\\\\\left(X_0\\left[\\begin{array}{cc}1 &amp; \\mathbf{0}_{N-1}^{\\top} \\\\\\mathbf{0}_{N-1} &amp; U\\end{array}\\right] \\Lambda\\left[\\begin{array}{cc}1 &amp; \\mathbf{0}_{N-1}^{\\top} \\\\\\mathbf{0}_{N-1} &amp; U^{\\top}\\end{array}\\right] X_0^{\\top}\\right)_{k, \\ell} \\leq 0 \\quad k \\neq \\ell, \\quad ( b^{\\prime} )\\\\\\Lambda=\\operatorname{diag}\\left(0, \\lambda_2, \\ldots, \\lambda_N\\right) \\succeq 0, \\text { (c) } \\\\\\operatorname{tr}(\\Lambda)=N \\in \\mathbb{R}_{*}^{+} \\text { (d) }\\end{cases}$$Further, the paper uses the log-barrier method to deal with the constraint ($b^{\\prime}$), as follows:","link":"/blog/2022/11/01/Reading/"}],"tags":[{"name":"Optimization","slug":"Optimization","link":"/blog/tags/Optimization/"},{"name":"Graph learning","slug":"Graph-learning","link":"/blog/tags/Graph-learning/"}],"categories":[{"name":"Paper Reading","slug":"Paper-Reading","link":"/blog/categories/Paper-Reading/"}],"pages":[{"title":"About Me","text":"Hi, thanks for visiting my blog, to know more information about me, you can browse my Personal Page","link":"/blog/about/index.html"}]}