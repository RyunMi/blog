<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Paper Reading 1 - Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation - Kerun&#039;s Blog</title><link rel="manifest" href="/blog/manifest.json"><meta name="application-name" content="Kerun&#039;s Blog"><meta name="msapplication-TileImage" content="/img/icon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Kerun&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Problem introduction The paper introduced an optimization problem to learn a graph from signals that are assumed to be smooth and admitting a sparse representation in the spectral domain of the graph."><meta property="og:type" content="blog"><meta property="og:title" content="Paper Reading 1 - Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation"><meta property="og:url" content="http://ryunmi.github.io/blog/2022/11/01/Reading/"><meta property="og:site_name" content="Kerun&#039;s Blog"><meta property="og:description" content="Problem introduction The paper introduced an optimization problem to learn a graph from signals that are assumed to be smooth and admitting a sparse representation in the spectral domain of the graph."><meta property="og:locale" content="en_US"><meta property="og:image" content="http://ryunmi.github.io/blog/gallery/covers/read1.png"><meta property="article:published_time" content="2022-11-01T13:34:22.000Z"><meta property="article:modified_time" content="2022-11-02T05:02:02.285Z"><meta property="article:author" content="Kerun Mi"><meta property="article:tag" content="Optimization"><meta property="article:tag" content="Graph learning"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://ryunmi.github.io/blog/gallery/covers/read1.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://ryunmi.github.io/blog/2022/11/01/Reading/"},"headline":"Paper Reading 1 - Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation","image":["http://ryunmi.github.io/blog/gallery/covers/read1.png"],"datePublished":"2022-11-01T13:34:22.000Z","dateModified":"2022-11-02T05:02:02.285Z","author":{"@type":"Person","name":"Kerun Mi"},"publisher":{"@type":"Organization","name":"Kerun's Blog","logo":{"@type":"ImageObject","url":"http://ryunmi.github.io/img/bigicon.png"}},"description":"Problem introduction The paper introduced an optimization problem to learn a graph from signals that are assumed to be smooth and admitting a sparse representation in the spectral domain of the graph."}</script><link rel="canonical" href="http://ryunmi.github.io/blog/2022/11/01/Reading/"><link rel="icon" href="/blog/img/icon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/blog/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/blog/"><img src="/blog/img/bigicon.png" alt="Kerun&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/blog/">Home</a><a class="navbar-item" href="/blog/about">About</a><a class="navbar-item" href="/blog/archives">Archives</a><a class="navbar-item" href="/blog/categories">Categories</a><a class="navbar-item" href="/blog/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/blog/gallery/covers/read1.png" alt="Paper Reading 1 - Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-11-01T13:34:22.000Z" title="2022/11/1 21:34:22">2022-11-01</time></span><span class="level-item"><a class="link-muted" href="/blog/categories/Paper-Reading/">Paper Reading</a></span></div></div><h1 class="title is-3 is-size-4-mobile">Paper Reading 1 - Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation</h1><div class="content"><h1 id="Problem-introduction"><a href="#Problem-introduction" class="headerlink" title="Problem introduction"></a>Problem introduction</h1><hr>
<p>The paper introduced an optimization problem to learn a graph from signals that are assumed to be smooth and admitting a sparse representation in the spectral domain of the graph.</p>
<span id="more"></span>
<blockquote>
<p><em>What is graph learning</em><br>Considering an undirected and weighted graph G=($\mathcal{V}$,$\mathcal{E}$) with no self-loops, where $\mathcal{V}=\lbrace 1,\dots,N \rbrace$ represent the node and $\mathcal{E}=\lbrace (i, j, w_{i j}), i, j \in \mathcal{V}\rbrace$ represent the edge with weights $w_{i j}$.<br>The paper focus on learning the Laplacian matrix $L$(Symmetric positive semi-define matrix)$=D - W$, where $D$ is the diagonal degree matrix and $W$ the weight matrix. $L’s$ eigenvalue decomposition is $L=X \Lambda X^{\top}$ with $\Lambda=\operatorname{diag}\left(\lambda_1, \ldots, \lambda_N\right)$ a diagonal matrix with the eigenvalues and $X=\left(x_1, \ldots, x_N\right)$ a matrix with the eigenvectors as columns.<br>A graph signal is defined as a function $y: \mathcal{V} \rightarrow \mathbb{R}^N$ that assigns a scalar value to each vertex.<br>The two concepts of smoothness and spectral representation in graph learning problems are explained as follows:</p>
</blockquote>
<blockquote>
<p><strong>Smoothness</strong><br>A graph signal $y$ is s-smooth with respect to the graph:\begin{equation} y^{\top} L y=\frac{1}{2} \sum_{i, j} w_{i j}\left(y_i-y_j\right)^2 \leq s\tag{1} \label{smooth} \end{equation} where $s\ge 0$ is smoothness level.</p>
</blockquote>
<blockquote>
<p><strong>Spectral sparsity</strong><br>A graph signal $y$ admits a $k$-sparse spectral representation with respect to the graph: $$|h|_0 \leq k$$ where $h=X^Ty$ for Graph Fourier Transform.</p>
</blockquote>
<p>From an intuitive point of view, smoothness means that the signal values between adjacent nodes are close, and k-sparse spectral representation means that in the overall graph, it can be divided into k clusters, and the signal values are smooth within the cluster and vary greatly between clusters.</p>
<h1 id="Abstract-optimization-problem"><a href="#Abstract-optimization-problem" class="headerlink" title="Abstract optimization problem"></a>Abstract optimization problem</h1><hr>
<p>$$\min _{H, X, \Lambda}|Y-X H|_F^2+\alpha\left|\Lambda^{1 / 2} H\right|_F^2+\beta|H|_S,\\<br>s.t. \begin{cases}<br>X^{\top} X=I_N, x_1=\frac{1}{\sqrt{N}} \mathbf{1}_N, \text { (a) } \\<br>(X \Lambda X^{\top})_{k,\ell} \leq 0 \quad k \neq \ell, \text { (b) } \\<br>\Lambda=\operatorname{diag} (0, \lambda_2, \ldots, \lambda_N) \succeq 0, \text { (c) } \\<br>\operatorname{tr}(\Lambda)=N \in \mathbb{R}_{*}^{+} \text { (d) } \\<br>\end{cases}<br>$$</p>
<ul>
<li>$|Y-X H|_F^2$ controls the distance of the new representation $XH$ to the observations $Y$.</li>
<li>From equation $(1)$, $\sum_i y^{(i) \top} L y^{(i)}=\operatorname{tr}\left(Y^{\top} L Y\right)=\left|L^{1 / 2} Y\right|_F^2$, then replacing $Y$ with $XH$, obtaing $\left|L^{1 / 2} X H\right|_F^2=\left|X \Lambda^{1 / 2} X^{\top} X H\right|_F^2=\left|X\right|_F^2 \left|\Lambda^{1 / 2}  H\right|_F^2=\operatorname{tr}(X^TX)\left|\Lambda^{1 / 2} H\right|_F^2=\left|\Lambda^{1 / 2} H\right|_F^2$. This objective item controls the smoothness of the new representation.</li>
<li>$\beta|H|_S$ is a sparsity regularization (as $h=X^Ty=X^{-1}y\Rightarrow Xh=y \Rightarrow XH \rightarrow Y$). The $|\cdot|_S$ here is often expressed as $\mathcal{l}_{1,2}$ (the sum of the $\mathcal{l}_{2}$-norm of each row of $H$) in the paper.</li>
<li>Constraints (a), (b), (c) originate from the properties of Laplacian matrix $L$.</li>
<li>Constraint (d) was as to impose structure in the learned graph while avoiding that the trivial solution $\widehat{\Lambda}=0$.</li>
</ul>
<h1 id="Reformulation-of-the-problem"><a href="#Reformulation-of-the-problem" class="headerlink" title="Reformulation of the problem"></a>Reformulation of the problem</h1><hr>
<blockquote>
<p><strong>Reformulation of the constraint $(a)$</strong><br>Given $X, X_0 \in \mathbb{R}^{N \times N}$ two orthogonal matrices, both having their first column equal to $\frac{1}{\sqrt{N}}\mathbf{1}_N$. And $$ X=X_0 \left[\begin{array}{cc} 1 &amp; \mathbf{0}_{N-1}^{\top} \\ \mathbf{0}_{N-1} &amp; {[X_0^{\top} X]_{2:, 2:}} \end{array}\right] $$ where $[X_0^{\top} X]_{2:, 2:} := U $ is in $\operatorname{Orth}(N-1)=\lbrace X \in \mathbb{R}^{N-1 \times N-1} \mid X^{\top} X=I_N \rbrace$.</p>
</blockquote>
<p>Constraint (a) is still satisfied after the above reformulation, and the optimization problem is transformed into the following:<br>$$\min_{H, U, \Lambda}\left|Y-X_0\left[\begin{array}{cc}1 &amp; \mathbf{0}_{N-1}^{\top} \\ \mathbf{0}_{N-1} &amp; U\end{array}\right] H\right|_F^2+\alpha\left|\Lambda^{1 / 2} H\right|_F^2+\beta|H|_S \triangleq f(H, U, \Lambda) \\<br>s.t. \begin{cases}<br>U^{\top} U=I_{N-1}, \quad ( a^{\prime} )\\<br>\left(X_0\left[\begin{array}{cc}1 &amp; \mathbf{0}_{N-1}^{\top} \\<br>\mathbf{0}_{N-1} &amp; U\end{array}\right] \Lambda\left[\begin{array}{cc}1 &amp; \mathbf{0}_{N-1}^{\top} \\<br>\mathbf{0}_{N-1} &amp; U^{\top}\end{array}\right] X_0^{\top}\right)_{k, \ell} \leq 0 \quad k \neq \ell, \quad ( b^{\prime} )\\<br>\Lambda=\operatorname{diag}\left(0, \lambda_2, \ldots, \lambda_N\right) \succeq 0, \text { (c) } \\<br>\operatorname{tr}(\Lambda)=N \in \mathbb{R}_{*}^{+} \text { (d) }<br>\end{cases}$$<br>Further, the paper uses the log-barrier method to deal with the constraint ($b^{\prime}$), as follows:</p>
<blockquote>
<p><strong>Reformulation of the constraint $(b^{\prime})$</strong><br>$$ \min_{H, U, \Lambda} f(H, U, \Lambda)-\frac{1}{t}\sum_{k=1}^{N-1} \sum_{\ell&gt;k}^N \log \left(-h(U, \Lambda)_{k, \ell}\right) \quad \text{ s.t. } \quad\left( a^{\prime}\right),( \mathrm{c}),(\mathrm{d}) $$ where $h(U, \Lambda)=X_0\left[\begin{array}{cc}1 &amp; \mathbf{0}_{N-1}^{\top} \\ \mathbf{0}_{N-1} &amp; U\end{array}\right] \Lambda\left[\begin{array}{cc}1 &amp; \mathbf{0}_{N-1}^{\top} \\ \mathbf{0}_{N-1} &amp; U\end{array}\right]^{\top} X_0^{\top}$,<br>with $\operatorname{dom}(\phi)=\lbrace (U, \Lambda) \in \mathbb{R}^{(N-1) \times(N-1)} \times \mathbb{R}^{N \times N} \mid \forall 1 \leq k&lt;\ell \leq N, h(U, \Lambda)_{k, \ell}&lt;0 \rbrace$.</p>
</blockquote>
<p>$\operatorname{dom}(\phi)$ just satisfies the constraint ($b^{\prime}$), and the paper mentioned that “This barrier function allows us to perform block-coordinate descent on three subproblems<br>that are easier to solve”.</p>
<h1 id="IGL-3SR-and-FGL-3SR"><a href="#IGL-3SR-and-FGL-3SR" class="headerlink" title="IGL-3SR and FGL-3SR"></a>IGL-3SR and FGL-3SR</h1><hr>
<h2 id="Iterative-Graph-Learning-for-Smooth-and-Sparse-Spectral-Representation"><a href="#Iterative-Graph-Learning-for-Smooth-and-Sparse-Spectral-Representation" class="headerlink" title="Iterative Graph Learning for Smooth and Sparse Spectral Representation"></a>Iterative Graph Learning for Smooth and Sparse Spectral Representation</h2><p>Spliting the problem in three partial minimizations and using a block-coordinate descent on $H$, $U$, and $\Lambda$.<br>Input:$ Y \in \mathbb{R}^{N \times n},\alpha,\beta$ and $t^{(0)},t_{max},\mu$ based on path-following method.</p>
<blockquote>
<p><strong>Optimization with respect to $H$</strong><br>For fixed $U$ and $\Lambda$, the minimization problem with respect to $H$ is: $$ \min_H|Y-X H|_F^2+\alpha\left|\Lambda^{1 / 2} H\right|_F^2+\beta|H|_S, \quad \text{ where } X=X_0\left[\begin{array}{cc} 1 &amp; \mathbf{0}_{N-1}^{\top} \\ \mathbf{0}_{N-1} &amp; U \end{array}\right] $$ The solution of problem above when $|\cdot|_S$ is set to $|\cdot|_{2,1}$ is: $$ \widehat{H}_{i,:}=\frac{1}{1+\alpha \lambda_i}\max\left(0,1-\frac{\beta}{2} \frac{1}{\left|\left(X^{\top} Y\right)_{i,:}\right|_2}\right)\left(X^{\top} Y\right)_{i,:} $$</p>
</blockquote>
<blockquote>
<p><strong>Optimization with respect to $\Lambda$</strong><br>For fixed $H$ and $U$, the minimization problem with respect to $\Lambda$ is: $$ \min_{\Lambda} \alpha \underbrace{\operatorname{tr}\left(H H^{\top} \Lambda\right)}_{\left|\Lambda^{1 / 2} H\right|_F^2}+\frac{1}{t} \phi(U, \Lambda) \quad \text{ s.t. } \begin{cases} \Lambda=\operatorname{diag}\left(0, \lambda_2, \ldots, \lambda_N\right) \succeq 0 \\ \operatorname{tr}(\Lambda)=N \in \mathbb{R}_{*}^{+} \end{cases} $$ Methods: CVXPY solver, interior-point, projected gradient descent.</p>
</blockquote>
<blockquote>
<p><strong>Optimization with respect to $U$</strong><br>For fixed $H$ and $\Lambda$, the minimization problem with respect to $U$ is:$$ \min_U\left|Y-X_0\left[\begin{array}{cc} 1 &amp; \mathbf{0}\mathbf{0}_{N-1}^{\top} \\ \mathbf{0}_{N-1} &amp; U\end{array}\right] H\right|_F^2+\frac{1}{t} \phi(U, \Lambda) \quad \text{ s.t. } \quad U^{\top} U=I_{(N-1)} $$ Approaching the solution: $$U\leftarrow retraction(U([(HY^TX_0)_{2:,2:}]U-U^T[(HY^TX_0)_{2:,2:}]^T))$$</p>
</blockquote>
<p>Considering the subproblem on $U$, the “retraction” represents a gradient descent method generalized to manifold–”consists in selecting, at each iteration, a search direction belonging to the tangent space of the manifold defined at the current point X, and then performing a descent along a curve of the manifold”.<br>On the convergence of IGL-3SR, any accumulation point $\left(H^{\infty},U^{\infty},\Lambda^{\infty}\right)$ of the sequence generated by IGL-3SR($\lbrace\left(H^{(\ell)}, U^{(\ell)},\Lambda^{(\ell)}\right)\rbrace_{\ell \geq 0}$) satisfies the KKT conditions of problem. The paper proves that $H^{\infty},U^{\infty},\Lambda^{\infty}$ all satisfy the KKT conditions of their respective sub-problems, and the Bolazano-Weierstrass theorem can obtain a sequence that must converge to $(H^{\infty},U^{\infty},\Lambda^{\infty})$. The paper cleverly sets three sub-problems for three sequence, all converge to $(H^{\infty},U^{\infty},\Lambda^{\infty})$, indicating that $(H^{\infty},U^{\infty},\Lambda^{\infty})$ is a KKT solution.</p>
<h2 id="Fast-Graph-Learning-for-Smooth-and-Sparse-Spectral-Representation"><a href="#Fast-Graph-Learning-for-Smooth-and-Sparse-Spectral-Representation" class="headerlink" title="Fast Graph Learning for Smooth and Sparse Spectral Representation"></a>Fast Graph Learning for Smooth and Sparse Spectral Representation</h2><p>Relying on a simplification of the minimization step in $X$ by removing the constraint (b).<br>The method of solving $H$ in FGL-3SR is the same as that in IGL-3SR. And ignoring the constraint (b) at the $X-step$ is to compute a closed-form solution to the optimization problem. Then using the learned X to optimize with respect to $\Lambda$.</p>
<blockquote>
<p><strong>Optimization with respect to $X$</strong><br>During the X-step: $$\min _X|Y-X H|_F^2 \quad s.t.\quad X^{\top} X=I_N, x_1=\frac{1}{\sqrt{N}} \mathbf{1}_N$$ Solving the X:<br>$M \leftarrow\left(X^{\top} Y H^{\top}\right)_{2:, 2:}, \left(P, D, Q^{\top}\right) \leftarrow \operatorname{SVD}(M), X \leftarrow X\left[\begin{array}{cc}1 &amp; \mathbf{0}_{N-1}^{\top} \\ \mathbf{0}_{N-1} &amp; P Q^{\top}\end{array}\right]$.</p>
</blockquote>
<blockquote>
<p><strong>Optimization with respect to $\Lambda$</strong><br>$$\min_{\Lambda} \alpha \underbrace{\operatorname{tr}\left(H H^{\top} \Lambda\right)}_{\left|\Lambda^{1 / 2} H\right|_F^2} \quad s.t. \begin{cases} \left(X \Lambda X^{\top}\right)_{i, j} \leq 0 \quad i \neq j, \\ \Lambda=\operatorname{diag}\left(0, \lambda_2, \ldots, \lambda_N\right) \succeq 0, \\ \operatorname{tr}(\Lambda)=N \in \mathbb{R}_{*}^{+}.\end{cases}.$$ Methods: interior-point, ellipsoid method.</p>
</blockquote>
<p>After the complexity analysis of the paper, the conclusion of “the empirical execution time of FGL-3SR is lower than IGL-3SR” is obtained.</p>
<h1 id="Numerical-simulation-and-others"><a href="#Numerical-simulation-and-others" class="headerlink" title="Numerical simulation and others"></a>Numerical simulation and others</h1><hr>
<p>Subsequently, the paper provide a probabilistic interpretation of the optimization program and test the algorithms and comparing it to state-of-the-art approaches on several synthetic and real datasets.<br>As stated in the paper “The findings of our empirical evaluation on synthetic data showed that the proposed approaches are as good or better performing than the reference state-of-the-art algorithms in term of reconstructing the unknown underlying graph and of computational cost (running time). Experiments on real-world benchmark use-cases suggest that our algorithms learn graphs that are useful and promising for any graph-based machine learning methodology, such as graph clustering and subsampling, etc” demonstrate the excellent performance of both algorithms.</p>
<hr>
<p><a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/v22/19-944.html">Original paper link</a></p>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/blog/tags/Optimization/">Optimization</a><a class="link-muted mr-2" rel="tag" href="/blog/tags/Graph-learning/">Graph learning</a></div><!--!--></article></div><!--!--><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/blog/img/avatar.jpg" alt="Kerun Mi"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Kerun Mi</p><p class="is-size-6 is-block">Mphil Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Nanjing-Jiangsu-China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Post</p><a href="/blog/archives"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/blog/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/blog/tags"><p class="title">2</p></a></div></div></nav><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/RyunMi"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/xiaomidemi"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Zhihu" href="https://www.zhihu.com/people/run-run-run-19"><i class="fab fa-zhihu"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/categories/Paper-Reading/"><span class="level-start"><span class="level-item">Paper Reading</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/blog/tags/Graph-learning/"><span class="tag">Graph learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/blog/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><figure class="media-left"><a class="image" href="/blog/2022/11/01/Reading/"><img src="/blog/gallery/thumbnails/read1small.png" alt="Paper Reading 1 - Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation"></a></figure><div class="media-content"><p class="date"><time dateTime="2022-11-01T13:34:22.000Z">2022-11-01</time></p><p class="title"><a href="/blog/2022/11/01/Reading/">Paper Reading 1 - Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation</a></p><p class="categories"><a href="/blog/categories/Paper-Reading/">Paper Reading</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/blog/"><img src="/blog/img/bigicon.png" alt="Kerun&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Kerun Mi</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'folded'
                }
            }
        };</script><script src="/blog/js/column.js"></script><script src="/blog/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/blog/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/blog/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/blog/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/blog/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>